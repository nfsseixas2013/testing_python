{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5207dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nilton/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "from unidecode import unidecode\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18353538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_texto(text):\n",
    "    \n",
    "    # Colocando todas as letras do texto em caixa baixa:\n",
    "    text = text.lower()\n",
    "    # Excluindo citações com @:\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "    # Excluindo acentuação das palavras:\n",
    "    text = unidecode(text)\n",
    "    # Excluindo html tags, como <strong></strong>:\n",
    "    text = re.sub('<[^<]+?>','', text)\n",
    "    # Excluindo os números:\n",
    "    text = ''.join(c for c in text if not c.isdigit())\n",
    "    # Excluindo URL's:\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', text)\n",
    "    # Excluindo pontuação:\n",
    "    text = ''.join(c for c in text if c not in punctuation)\n",
    "    \n",
    "    # Retornando o texto tratado tokenizado:\n",
    "    tokens = word_tokenize(text)\n",
    "    resultado = ' '.join(word for word in tokens)\n",
    "    return resultado\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "920fca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"\"\"\n",
    "<strong>Olá</strong> @usuario, vamos testar a função #clean_text?\n",
    "Caso tenha dúvidas, uma boa pesquisa no www.google.com pode ajudar!\n",
    "Mesmo que você tenha que pesquisar 100 vezes!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e83cbadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_limpo = limpar_texto(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3128862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ola vamos testar a funcao cleantext caso tenha duvidas uma boa pesquisa no pode ajudar mesmo que voce tenha que pesquisar vezes'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b1954",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remoção de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6efc8db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'à',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as',\n",
       " 'às',\n",
       " 'até',\n",
       " 'com',\n",
       " 'como',\n",
       " 'da',\n",
       " 'das',\n",
       " 'de',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'dele',\n",
       " 'deles',\n",
       " 'depois',\n",
       " 'do',\n",
       " 'dos',\n",
       " 'e',\n",
       " 'é',\n",
       " 'ela',\n",
       " 'elas',\n",
       " 'ele',\n",
       " 'eles',\n",
       " 'em',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eram',\n",
       " 'éramos',\n",
       " 'essa',\n",
       " 'essas',\n",
       " 'esse',\n",
       " 'esses',\n",
       " 'esta',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estão',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'estava',\n",
       " 'estavam',\n",
       " 'estávamos',\n",
       " 'este',\n",
       " 'esteja',\n",
       " 'estejam',\n",
       " 'estejamos',\n",
       " 'estes',\n",
       " 'esteve',\n",
       " 'estive',\n",
       " 'estivemos',\n",
       " 'estiver',\n",
       " 'estivera',\n",
       " 'estiveram',\n",
       " 'estivéramos',\n",
       " 'estiverem',\n",
       " 'estivermos',\n",
       " 'estivesse',\n",
       " 'estivessem',\n",
       " 'estivéssemos',\n",
       " 'estou',\n",
       " 'eu',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'for',\n",
       " 'fora',\n",
       " 'foram',\n",
       " 'fôramos',\n",
       " 'forem',\n",
       " 'formos',\n",
       " 'fosse',\n",
       " 'fossem',\n",
       " 'fôssemos',\n",
       " 'fui',\n",
       " 'há',\n",
       " 'haja',\n",
       " 'hajam',\n",
       " 'hajamos',\n",
       " 'hão',\n",
       " 'havemos',\n",
       " 'haver',\n",
       " 'hei',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houver',\n",
       " 'houvera',\n",
       " 'houverá',\n",
       " 'houveram',\n",
       " 'houvéramos',\n",
       " 'houverão',\n",
       " 'houverei',\n",
       " 'houverem',\n",
       " 'houveremos',\n",
       " 'houveria',\n",
       " 'houveriam',\n",
       " 'houveríamos',\n",
       " 'houvermos',\n",
       " 'houvesse',\n",
       " 'houvessem',\n",
       " 'houvéssemos',\n",
       " 'isso',\n",
       " 'isto',\n",
       " 'já',\n",
       " 'lhe',\n",
       " 'lhes',\n",
       " 'mais',\n",
       " 'mas',\n",
       " 'me',\n",
       " 'mesmo',\n",
       " 'meu',\n",
       " 'meus',\n",
       " 'minha',\n",
       " 'minhas',\n",
       " 'muito',\n",
       " 'na',\n",
       " 'não',\n",
       " 'nas',\n",
       " 'nem',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nós',\n",
       " 'nossa',\n",
       " 'nossas',\n",
       " 'nosso',\n",
       " 'nossos',\n",
       " 'num',\n",
       " 'numa',\n",
       " 'o',\n",
       " 'os',\n",
       " 'ou',\n",
       " 'para',\n",
       " 'pela',\n",
       " 'pelas',\n",
       " 'pelo',\n",
       " 'pelos',\n",
       " 'por',\n",
       " 'qual',\n",
       " 'quando',\n",
       " 'que',\n",
       " 'quem',\n",
       " 'são',\n",
       " 'se',\n",
       " 'seja',\n",
       " 'sejam',\n",
       " 'sejamos',\n",
       " 'sem',\n",
       " 'ser',\n",
       " 'será',\n",
       " 'serão',\n",
       " 'serei',\n",
       " 'seremos',\n",
       " 'seria',\n",
       " 'seriam',\n",
       " 'seríamos',\n",
       " 'seu',\n",
       " 'seus',\n",
       " 'só',\n",
       " 'somos',\n",
       " 'sou',\n",
       " 'sua',\n",
       " 'suas',\n",
       " 'também',\n",
       " 'te',\n",
       " 'tem',\n",
       " 'tém',\n",
       " 'temos',\n",
       " 'tenha',\n",
       " 'tenham',\n",
       " 'tenhamos',\n",
       " 'tenho',\n",
       " 'terá',\n",
       " 'terão',\n",
       " 'terei',\n",
       " 'teremos',\n",
       " 'teria',\n",
       " 'teriam',\n",
       " 'teríamos',\n",
       " 'teu',\n",
       " 'teus',\n",
       " 'teve',\n",
       " 'tinha',\n",
       " 'tinham',\n",
       " 'tínhamos',\n",
       " 'tive',\n",
       " 'tivemos',\n",
       " 'tiver',\n",
       " 'tivera',\n",
       " 'tiveram',\n",
       " 'tivéramos',\n",
       " 'tiverem',\n",
       " 'tivermos',\n",
       " 'tivesse',\n",
       " 'tivessem',\n",
       " 'tivéssemos',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tuas',\n",
       " 'um',\n",
       " 'uma',\n",
       " 'você',\n",
       " 'vocês',\n",
       " 'vos']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be5eaa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = word_tokenize(\"o Brasil é penta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5c2fe6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o', 'Brasil', 'é', 'penta']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11dd0f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords =  list(set(stopwords.words('portuguese') + list(STOP_WORDS))) \n",
    "def remove_stop_words(texts,stopwords):\n",
    "    new_texts = \"\"\n",
    "    new_texts = ' '.join(word for word in word_tokenize(texts) if word not in stopwords)\n",
    "    return new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcba3f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ola vamos testar funcao cleantext caso duvidas pesquisa ajudar voce pesquisar'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stop_words(texto_limpo,stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d60c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lematização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8258b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizacao(words):\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")\n",
    "    resultado = ' '.join(word.lemma_ for word in nlp(words))\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "114e1915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ola ir testar funcao cleantext casar duvidar pesquisar ajudar voce pesquisar'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizacao(\"ola vamos testar funcao cleantext caso duvidas pesquisa ajudar voce pesquisar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dafc1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"(115, 11500)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5c52f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(115, 11500)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0b265a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('(', '1', '1', '5', ',', ' ', '1', '1', '5', '0', '0', ')')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c49dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "{}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
